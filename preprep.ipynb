{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2499b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Providers from PR: 69\n",
      "Unmatched Providers from PR: 33\n",
      "Matched Providers from FDR: 67\n",
      "Unmatched Providers from FDR: 4\n",
      "['Shnita Wiley', 'Chekeli Daniels', 'Mya Williams', 'Jessica Bonilla', 'Summer Williams', 'Cassandra Westbrook']\n",
      "['Jillian Mills']\n",
      "['Leah Gravesen', 'Kiryn Evans', 'Connie Zurski', 'Olumide Adeyemo', 'Miranda McMahan', 'Emily Hamilton', 'Christina Hay', 'Amit Parkhe', 'Sherry Boyd', 'Paige  Dennis', 'Sasha Moredock', 'Stephen Skimehorn']\n",
      "['Aretha Johnson', 'Tamara  Baird', 'Jeanea Morris', 'Katelin Breuning', 'Ashley McPhillips', 'Brandi Cassaday']\n",
      "['Eli Perez', 'Tashica Bruce']\n",
      "['Hina Patel', 'Roshana Brown']\n",
      "['Adejimi Carrington', 'Vera Dankwah', 'Ahmad Almawaldi', 'Germaine Henderson', 'Nicole  Cartier', 'Melissa Townsend']\n",
      "['Kevin Widdis']\n",
      "['Bobby Hill']\n",
      "['Shawanda Minor', 'Shawn Gorkiewicz', 'Kathryn  McAdam', 'Christopher Briggs', 'Anna OKeefe', 'Brenda Buchanan']\n",
      "['Abdulilah Obeid', 'Marina Memon', 'Hala Hojeije', 'Patricia Lee', 'Hana Bazzi', 'Hussein Fawaz', 'Catherine Yergenson']\n",
      "['Kendra Khan']\n",
      "['Cara Peterson']\n",
      "['Adam Geller']\n",
      "['Ashley Weis', 'Katelyn Forsell', 'Arianne Davis']\n",
      "['Krystal Jones-Shephard', 'Chanta McKinley']\n",
      "['Stephanie Duda', 'Tori Miller', 'Victoria Francis', 'Jessica Berardi']\n",
      "['Rachel Kempa']\n",
      "['Dana Fields']\n",
      "['Elysa  Maunder']\n",
      "[]\n",
      "['Amy Cervantes', 'Mareah Lucio', 'Courtney  Briney']\n",
      "[]\n",
      "['Rebekah Serrato', 'Jennifer McGinnis', 'Nicole Stringfellow']\n",
      "['Wanalee Betts']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from rapidfuzz import process, fuzz\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = str(s).lower().strip()\n",
    "    s = re.sub(r\"[^\\w\\s'-]\", \" \", s)\n",
    "    s = s.replace(\"-\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def _parse_name(s: str):\n",
    "    s = _norm(s)\n",
    "    parts = s.split()\n",
    "    if not parts:\n",
    "        return {\"first\":\"\", \"middles\":[], \"last\":\"\", \"tokens\":set()}\n",
    "    prefixes = {\"dr\",\"mr\",\"mrs\",\"ms\",\"miss\"}\n",
    "    suffixes = {\"jr\",\"sr\",\"ii\",\"iii\",\"iv\"}\n",
    "    parts = [p for p in parts if p not in prefixes|suffixes]\n",
    "    if not parts:\n",
    "        return {\"first\":\"\", \"middles\":[], \"last\":\"\", \"tokens\":set()}\n",
    "    first = parts[0]\n",
    "    last = parts[-1] if len(parts) > 1 else \"\"\n",
    "    middles = parts[1:-1] if len(parts) > 2 else []\n",
    "    return {\"first\": first, \"middles\": middles, \"last\": last, \"tokens\": set(parts)}\n",
    "\n",
    "def _first_match_score(a_first: str, b_first: str) -> float:\n",
    "    if not a_first or not b_first:\n",
    "        return 0.0\n",
    "    a_first = a_first.strip().lower()\n",
    "    b_first = b_first.strip().lower()\n",
    "    # initial vs full name\n",
    "    if a_first[0] == b_first[0] and (len(a_first) == 1 or len(b_first) == 1):\n",
    "        return 95.0\n",
    "    # fallback similarity\n",
    "    return float(fuzz.ratio(a_first, b_first))\n",
    "\n",
    "def _last_match_score(a_last: str, b_last: str) -> float:\n",
    "    if not a_last or not b_last:\n",
    "        return 0.0\n",
    "    a = a_last.replace(\"'\", \"\")\n",
    "    b = b_last.replace(\"'\", \"\")\n",
    "    return float(max(fuzz.ratio(a, b), fuzz.token_sort_ratio(a, b)))\n",
    "\n",
    "def name_similarity(a: str, b: str, *, score_cutoff: float = 0.0) -> float:\n",
    "    pa, pb = _parse_name(a), _parse_name(b)\n",
    "    last = _last_match_score(pa[\"last\"], pb[\"last\"])\n",
    "    if last < 75:\n",
    "        return 0.0\n",
    "    first = _first_match_score(pa[\"first\"], pb[\"first\"])\n",
    "    middle = 0.0\n",
    "    if pa[\"middles\"] and pb[\"middles\"]:\n",
    "        middle = float(fuzz.token_set_ratio(\" \".join(pa[\"middles\"]), \" \".join(pb[\"middles\"])))\n",
    "    full = float(fuzz.token_set_ratio(_norm(a), _norm(b)))\n",
    "    weighted = 0.7*last + 0.25*first + 0.05*middle\n",
    "    score = max(weighted, full if last >= 85 else weighted)\n",
    "    return score if score >= score_cutoff else 0.0\n",
    "\n",
    "def match_providers(providers, org_providers, threshold=76):\n",
    "    matched = []\n",
    "    unmatched_map = {}  # providers_norm -> best unmatched\n",
    "\n",
    "    matched_providers_set = set()  # to avoid re-matching same provider\n",
    "    already_matched_org_providers_names = set()\n",
    "\n",
    "    providers_map = { _norm(p): p for p in providers }\n",
    "    org_providers_map = { _norm(p): p for p in org_providers }\n",
    "    comp_norm = list(providers_map.keys())\n",
    "    cap_norm = list(org_providers_map.keys())\n",
    "\n",
    "    def last_token(s):\n",
    "        parts = _norm(s).split()\n",
    "        return parts[-1] if parts else \"\"\n",
    "\n",
    "    # simple blocking\n",
    "    cap_by_last, cap_by_initial = {}, {}\n",
    "    for n in cap_norm:\n",
    "        lt = last_token(n)\n",
    "        cap_by_last.setdefault(lt, []).append(n)\n",
    "        cap_by_initial.setdefault(lt[:1], []).append(n)\n",
    "\n",
    "    for cr in comp_norm:\n",
    "        original_cr = providers_map[cr]\n",
    "        if original_cr in matched_providers_set:\n",
    "            continue\n",
    "\n",
    "        best_name = None\n",
    "        best_score = -1.0\n",
    "        matched_here = False\n",
    "\n",
    "        for current_threshold in range(90, threshold - 1, -5):\n",
    "            lt = last_token(cr)\n",
    "            cand = set(cap_by_last.get(lt, [])) | set(cap_by_initial.get(lt[:1], []))\n",
    "            if not cand:\n",
    "                cand = set(cap_norm)\n",
    "            cand = [c for c in cand if c not in already_matched_org_providers_names]\n",
    "            if not cand:\n",
    "                continue\n",
    "\n",
    "            best = process.extractOne(cr, cand, scorer=name_similarity)\n",
    "            if best is None:\n",
    "                continue\n",
    "\n",
    "            match_name, score, _ = best\n",
    "            score = float(score)\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_name = match_name\n",
    "\n",
    "            if score >= current_threshold:\n",
    "                matched.append({\n",
    "                    'providers': original_cr,\n",
    "                    'org_providers_name': org_providers_map[match_name],\n",
    "                    'score': score\n",
    "                })\n",
    "                matched_providers_set.add(original_cr)\n",
    "                already_matched_org_providers_names.add(match_name)\n",
    "                matched_here = True\n",
    "                break\n",
    "\n",
    "        if not matched_here:\n",
    "            unmatched_map[cr] = {\n",
    "                'providers': original_cr,\n",
    "                'org_providers_name': org_providers_map[best_name] if best_name and best_score > 0 else None,\n",
    "                'score': best_score if best_score > 0 else 0.0\n",
    "            }\n",
    "\n",
    "    unmatched = sorted(unmatched_map.values(), key=lambda x: x['score'], reverse=True)\n",
    "    return {\"matched\": matched, \"unmatched\": unmatched}\n",
    "def preprocess_PR_FDR_xlsx(PR_df, FDR_df):\n",
    "\n",
    "    def _to_numeric_series(s: pd.Series) -> pd.Series:\n",
    "        cleaned = (\n",
    "            s.astype(str).str.strip()\n",
    "             .str.replace(r'[,\\s$]', '', regex=True)\n",
    "             .str.replace(r'^\\((.*)\\)$', r'-\\1', regex=True)\n",
    "        )\n",
    "        return pd.to_numeric(cleaned, errors='coerce')\n",
    "    PR_df = PR_df.loc[:, [\"NAME\",\"REGULAR EARNINGS\",\"GROSS PAY\"]].copy()\n",
    "    PR_df.dropna(subset=[\"NAME\"], inplace=True)\n",
    "    new_PR_sum=PR_df.groupby(\"NAME\").agg({\"GROSS PAY\":\"sum\",\"REGULAR EARNINGS\":\"sum\"}).reset_index()\n",
    "    # rename columns\n",
    "    new_PR_sum = new_PR_sum.rename(columns={\"NAME\":\"Personnel\", \"REGULAR EARNINGS\":\"Earnings_Reg\", \"GROSS PAY\":\"Gross\"})\n",
    "    \n",
    "    # Ensure numeric types for payroll\n",
    "    new_PR_sum[\"Earnings_Reg\"] = _to_numeric_series(new_PR_sum[\"Earnings_Reg\"])\n",
    "    new_PR_sum[\"Gross\"] = _to_numeric_series(new_PR_sum[\"Gross\"])\n",
    "\n",
    "    # FDR cleanup and numeric (copy slice first)\n",
    "    FDR_df = FDR_df.loc[:, [\"PROVIDER\",\"NET CHARGES\",\"NET RECEIPTS\"]].copy()\n",
    "    FDR_df[\"NET CHARGES\"] = _to_numeric_series(FDR_df[\"NET CHARGES\"])\n",
    "    FDR_df[\"NET RECEIPTS\"] = _to_numeric_series(FDR_df[\"NET RECEIPTS\"])\n",
    "\n",
    "    FDR_df = FDR_df[~((FDR_df[\"NET CHARGES\"] == 0.0) & (FDR_df[\"NET RECEIPTS\"] == 0.0))]\n",
    "    FDR_df.drop_duplicates(subset=[\"PROVIDER\"], inplace=True)\n",
    "    new_PR_sum.drop_duplicates(subset=[\"Personnel\"], inplace=True)\n",
    "    return new_PR_sum, FDR_df\n",
    "def preprocess_PR_FDR_xls(PR_df, FDR_df):\n",
    "        # Merge first 2 rows into one header row\n",
    "    header1 = PR_df.iloc[0].fillna(\"\")\n",
    "    header2 = PR_df.iloc[1].fillna(\"\")\n",
    "\n",
    "    # Build combined header\n",
    "    new_cols = []\n",
    "    for h1, h2 in zip(header1, header2):\n",
    "        if h1 and h2:\n",
    "            new_cols.append(f\"{h1}_{h2}\".strip())\n",
    "        elif h1:  # only top header\n",
    "            new_cols.append(h1.strip())\n",
    "        else:     # only sub header\n",
    "            new_cols.append(h2.strip())\n",
    "    PR_df = PR_df[3:]\n",
    "    PR_df.columns = new_cols\n",
    "    PR_df = PR_df.reset_index(drop=True)\n",
    "\n",
    "    # Optional: clean names\n",
    "    PR_df.columns = (\n",
    "        PR_df.columns.str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "                 .str.replace(r\"[^\\w]\", \"\", regex=True)\n",
    "                 .str.replace(\"__\", \"_\")\n",
    "    )\n",
    "\n",
    "    # take a copy of the slice to avoid SettingWithCopy\n",
    "    PR_df = PR_df.loc[:, [\"Personnel\",\"Earnings_Reg\",\"Gross\"]].copy()\n",
    "    PR_df.dropna(inplace=True)\n",
    "    PR_df['Personnel'] = PR_df['Personnel'].str.split(\"\\n\").str[0]\n",
    "    for row in PR_df.itertuples():\n",
    "        personnel = row.Personnel\n",
    "        personnel = personnel.strip().split(\",\")\n",
    "        personnel = personnel[1].strip() + \" \" + personnel[0].strip()\n",
    "        PR_df.at[row.Index, 'Personnel'] = personnel\n",
    "    def _to_numeric_series(s: pd.Series) -> pd.Series:\n",
    "        cleaned = (\n",
    "            s.astype(str).str.strip()\n",
    "             .str.replace(r'[,\\s$]', '', regex=True)\n",
    "             .str.replace(r'^\\((.*)\\)$', r'-\\1', regex=True)\n",
    "        )\n",
    "        return pd.to_numeric(cleaned, errors='coerce')\n",
    "\n",
    "    # Ensure numeric types for payroll\n",
    "    PR_df[\"Earnings_Reg\"] = _to_numeric_series(PR_df[\"Earnings_Reg\"])\n",
    "    PR_df[\"Gross\"] = _to_numeric_series(PR_df[\"Gross\"])\n",
    "\n",
    "    # FDR cleanup and numeric (copy slice first)\n",
    "    FDR_df = FDR_df.loc[:, [\"PROVIDER\",\"NET CHARGES\",\"NET RECEIPTS\"]].copy()\n",
    "    FDR_df[\"NET CHARGES\"] = _to_numeric_series(FDR_df[\"NET CHARGES\"])\n",
    "    FDR_df[\"NET RECEIPTS\"] = _to_numeric_series(FDR_df[\"NET RECEIPTS\"])\n",
    "\n",
    "    FDR_df = FDR_df[~((FDR_df[\"NET CHARGES\"] == 0.0) & (FDR_df[\"NET RECEIPTS\"] == 0.0))]\n",
    "    FDR_df.drop_duplicates(subset=[\"PROVIDER\"], inplace=True)\n",
    "    PR_df.drop_duplicates(subset=[\"Personnel\"], inplace=True)\n",
    "    return PR_df, FDR_df\n",
    "\n",
    "\n",
    "def build_metadata(charge_capture_df, company_roster_df, PR_df, FDR_df):\n",
    "    CC_used_col = [\"Provider\", \"CPT Codes\", \"Charge Status\"]\n",
    "    CR_used_col = ['Name', \"Manager\", 'State/Region']\n",
    "\n",
    "    CC_filtered_df = charge_capture_df[CC_used_col]\n",
    "    CR_filtered_df = company_roster_df[CR_used_col]\n",
    "\n",
    "    # Rename column safely\n",
    "    CR_filtered_df = CR_filtered_df.rename(columns={'Name': 'Provider'})\n",
    "\n",
    "    grouped_CC = CC_filtered_df.groupby('Provider')\n",
    "    grouped_CR = CR_filtered_df.groupby('State/Region')\n",
    "    cpt_pattern = re.compile(r'\\b993\\d{2}\\b')  # Example pattern for 5-digit codes starting with 9930\n",
    "    Margin_df = pd.DataFrame({\n",
    "        'Clinician_Name': pd.Series(dtype='object'),\n",
    "        'Manager': pd.Series(dtype='object'),\n",
    "        'Gross_Consents': pd.Series(dtype='int'),\n",
    "        'Gross_Encounters': pd.Series(dtype='int'),\n",
    "        'Drafted_Encounters': pd.Series(dtype='int'), \n",
    "        'Collections': pd.Series(dtype='float'), \n",
    "        'Anticipated_Collections': pd.Series(dtype='float'),\n",
    "        'Regular_Pay': pd.Series(dtype='float'),\n",
    "        'Regular_Margin': pd.Series(dtype='float'),\n",
    "        'Gross_Pay': pd.Series(dtype='float'),\n",
    "        'Net_Margin': pd.Series(dtype='float'),\n",
    "        'Anticipated_Net_Margin': pd.Series(dtype='float'),\n",
    "        'region': pd.Series(dtype='object'),\n",
    "    })\n",
    "    for name, group in grouped_CC:\n",
    "        group = group.copy()\n",
    "        group['CPT Codes'] = group['CPT Codes'].astype(str).str.split(',')\n",
    "        group = group.explode('CPT Codes')\n",
    "        group['CPT Codes'] = group['CPT Codes'].str.strip()\n",
    "        group['Charge Status'] = group['Charge Status'].str.strip().str.lower()\n",
    "        # CCM Counts\n",
    "        CCM_counts =group[group['CPT Codes']==\"44444\"]\n",
    "        CCM_counts = CCM_counts['CPT Codes'].count()\n",
    "\n",
    "        # Filter rows where 'CPT Codes' is in the target list and 'Charge Status' is 'draft'\n",
    "        draft_counts = group[\n",
    "            (group['CPT Codes'].str.match(cpt_pattern, na=False)) & \n",
    "            (group['Charge Status'] == 'draft')\n",
    "        ]\n",
    "\n",
    "        # Count the rows\n",
    "        draft_counts = draft_counts['Charge Status'].count()\n",
    "        # Gross Encounters    \n",
    "        cpt_grouped = group[group['CPT Codes'].str.match(cpt_pattern, na=False)]\n",
    "        gross_encounters = cpt_grouped['CPT Codes'].count()\n",
    "\n",
    "        new_row = {'Clinician_Name': name,\n",
    "                    'Manager': None,\n",
    "                    'Gross_Consents': CCM_counts,\n",
    "                    'Gross_Encounters': gross_encounters,\n",
    "                    'Drafted_Encounters': draft_counts,\n",
    "                    'Collections': 0.0,  # Placeholder for Collections\n",
    "                    'Anticipated_Collections': 0.0,  # Placeholder for Anticipated Collections\n",
    "                    'Regular_Pay': 0.0,  # Placeholder for Regular Pay\n",
    "                    'Regular_Margin': 0.0,  # Placeholder for Regular Margin\n",
    "                    'Gross_Pay': 0.0,  # Placeholder for Gross Pay\n",
    "                    'Net_Margin': 0.0,  # Placeholder for Net Margin\n",
    "                    'Anticipated_Net_Margin': 0.0,  # Placeholder for Anticipated Net Margin\n",
    "                    \"region\": None,\n",
    "                    }   \n",
    "\n",
    "        Margin_df.loc[len(Margin_df)] = new_row\n",
    "    \n",
    "    \n",
    "    PR_names = PR_df['Personnel'].tolist()\n",
    "    FDR_names = FDR_df['PROVIDER'].tolist()\n",
    "    Margin_df_names= Margin_df['Clinician_Name'].tolist()\n",
    "\n",
    "    # Match providers from PR with Margin_df\n",
    "    PR_result = match_providers(PR_names, Margin_df_names, threshold=85)\n",
    "    matched_providers_PR = PR_result['matched']\n",
    "    unmatched_providers_PR = PR_result['unmatched'] \n",
    "    print(f\"Matched Providers from PR: {len(matched_providers_PR)}\")\n",
    "    print(f\"Unmatched Providers from PR: {len(unmatched_providers_PR)}\")\n",
    "    # Update Margin_df with PR data\n",
    "    for row in matched_providers_PR:\n",
    "        clinician_name = row['providers']\n",
    "        charge_capture_name = row['org_providers_name']\n",
    "        score = row['score']\n",
    "\n",
    "        reg_series = PR_df.loc[PR_df['Personnel'] == clinician_name, \"Earnings_Reg\"]\n",
    "        reg_value = float(reg_series.iloc[0]) if not reg_series.empty and pd.notna(reg_series.iloc[0]) else np.nan\n",
    "        Margin_df.loc[Margin_df['Clinician_Name'] == charge_capture_name, 'Regular_Pay'] = reg_value\n",
    "\n",
    "        gross_series = PR_df.loc[PR_df['Personnel'] == clinician_name, \"Gross\"]\n",
    "        gross_value = float(gross_series.iloc[0]) if not gross_series.empty and pd.notna(gross_series.iloc[0]) else np.nan\n",
    "        Margin_df.loc[Margin_df['Clinician_Name'] == charge_capture_name, 'Gross_Pay'] = gross_value\n",
    "\n",
    "\n",
    "    # Match providers from FDR with Margin_df\n",
    "    FDR_result = match_providers(FDR_names, Margin_df_names, threshold=85)\n",
    "    matched_providers_FDR = FDR_result['matched']\n",
    "    unmatched_providers_FDR = FDR_result['unmatched']\n",
    "    print(f\"Matched Providers from FDR: {len(matched_providers_FDR)}\")\n",
    "    print(f\"Unmatched Providers from FDR: {len(unmatched_providers_FDR)}\")\n",
    "\n",
    "    # Update Margin_df with FDR data\n",
    "    for row in matched_providers_FDR:\n",
    "        clinician_name = row['providers']\n",
    "        charge_capture_name = row['org_providers_name']\n",
    "        score = row['score']\n",
    "\n",
    "        # Assign scalars, not Series\n",
    "        charges_series = FDR_df.loc[FDR_df['PROVIDER'] == clinician_name, \"NET CHARGES\"]\n",
    "        charges_value = float(charges_series.iloc[0]) if not charges_series.empty and pd.notna(charges_series.iloc[0]) else np.nan\n",
    "        Margin_df.loc[Margin_df['Clinician_Name'] == charge_capture_name, 'Anticipated_Collections'] = charges_value\n",
    "\n",
    "        receipts_series = FDR_df.loc[FDR_df['PROVIDER'] == clinician_name, \"NET RECEIPTS\"]\n",
    "        receipts_value = float(receipts_series.iloc[0]) if not receipts_series.empty and pd.notna(receipts_series.iloc[0]) else np.nan\n",
    "        Margin_df.loc[Margin_df['Clinician_Name'] == charge_capture_name, 'Collections'] = receipts_value\n",
    "        # Regular_Margin \"(regular pay column / collections)*100\"\n",
    "        if not pd.isna(receipts_value) and receipts_value != 0:\n",
    "            regular_pay = Margin_df.loc[Margin_df['Clinician_Name'] == charge_capture_name, 'Regular_Pay'].values[0]\n",
    "            if not pd.isna(regular_pay):\n",
    "                regular_margin = (receipts_value / regular_pay ) * 100 if regular_pay != 0 else 0\n",
    "                Margin_df.loc[Margin_df['Clinician_Name'] == charge_capture_name, 'Regular_Margin'] = round(regular_margin,2)\n",
    "                \n",
    "        # Net_Margin \"(gross pay column / collections)*100\"\n",
    "        if not pd.isna(charges_value) and charges_value != 0:\n",
    "            gross_pay = Margin_df.loc[Margin_df['Clinician_Name'] == charge_capture_name, 'Gross_Pay'].values[0]\n",
    "            if not pd.isna(gross_pay):\n",
    "                Anticipated_Net_Margin = (charges_value/ gross_pay ) * 100 if gross_pay != 0 else 0\n",
    "                net_margin = (receipts_value / gross_pay) * 100 if gross_pay != 0 else 0\n",
    "                Margin_df.loc[Margin_df['Clinician_Name'] == charge_capture_name, 'Anticipated_Net_Margin'] = round(Anticipated_Net_Margin, 2)\n",
    "                Margin_df.loc[Margin_df['Clinician_Name'] == charge_capture_name, 'Net_Margin'] = round(net_margin, 2)\n",
    "\n",
    "    \n",
    "\n",
    "    unmatched_providers = []\n",
    "    matched_providers = []\n",
    "    Regional_Dashboard=pd.DataFrame(columns=[\"Region\",\"RCS\",\"RDO\",\"Gross Encounters\",\"Gross Consents\",\"Gross Drafts\",\"Regular_margin\",\"Gross_margin\",\"Anticipated_Net_Margin\"])\n",
    "\n",
    "    for name, group in grouped_CR:\n",
    "        manager_group=group.groupby('Manager')\n",
    "        for manager_name, manager_group in manager_group:\n",
    "            Region_list=Margin_df['Clinician_Name']\n",
    "            manager_group_list = manager_group['Provider']\n",
    "            result=match_providers(manager_group_list, Region_list, threshold=85)\n",
    "            unmatched_providers.extend(result['unmatched'])\n",
    "            matched_providers.extend(result['matched'])\n",
    "            matched_providers_CC = [item['org_providers_name'] for item in result['matched']]\n",
    "            Region_Gross_Encounters = Margin_df[Margin_df['Clinician_Name'].isin([item['org_providers_name'] for item in result['matched']])]['Gross_Encounters'].sum()\n",
    "            Region_Gross_Consents = Margin_df[Margin_df['Clinician_Name'].isin([item['org_providers_name'] for item in result['matched']])]['Gross_Consents'].sum()\n",
    "            Region_Gross_Drafts = Margin_df[Margin_df['Clinician_Name'].isin([item['org_providers_name'] for item in result['matched']])]['Drafted_Encounters'].sum()\n",
    "            Region_Regular_margin = Margin_df[Margin_df['Clinician_Name'].isin([item['org_providers_name'] for item in result['matched']])]['Regular_Margin'].mean()\n",
    "            Region_Gross_margin = Margin_df[Margin_df['Clinician_Name'].isin([item['org_providers_name'] for item in result['matched']])]['Net_Margin'].mean()\n",
    "            Region_Anticipated_Net_Margin = Margin_df[Margin_df['Clinician_Name'].isin([item['org_providers_name'] for item in result['matched']])]['Anticipated_Net_Margin'].mean()  \n",
    "            # Create a new row for the Regional Dashboard\n",
    "            print(matched_providers_CC)\n",
    "            if not matched_providers_CC:\n",
    "                continue  # Skip if no matched providers  \n",
    "            new_row = {\n",
    "                \"Region\": name,\n",
    "                \"RCS\": manager_name,\n",
    "                \"RDO\": matched_providers_CC,\n",
    "                \"Gross Encounters\": Region_Gross_Encounters,\n",
    "                \"Gross Consents\": Region_Gross_Consents,\n",
    "                \"Gross Drafts\": Region_Gross_Drafts,\n",
    "                \"Regular_margin\": Region_Regular_margin,  # Placeholder for Net Margin\n",
    "                \"Gross_margin\": Region_Gross_margin,  # Placeholder for Gross Margin\n",
    "                \"Anticipated_Net_Margin\": Region_Anticipated_Net_Margin,  # Placeholder for Anticipated Net Margin\n",
    "            }\n",
    "            Regional_Dashboard.loc[len(Regional_Dashboard)] = new_row\n",
    "    Regional_Dashboard.columns = Regional_Dashboard.columns.str.replace(' ', '_')\n",
    "    Margin_df.columns = Margin_df.columns.str.replace(' ', '_')\n",
    "    for row in Regional_Dashboard.itertuples():\n",
    "        region = row.Region\n",
    "        Clinician_list = row.RDO\n",
    "        manager= row.RCS    \n",
    "        # Update the 'region' column for all clinicians in the list\n",
    "        Margin_df.loc[Margin_df['Clinician_Name'].isin(Clinician_list), 'region'] = region\n",
    "        Margin_df.loc[Margin_df['Clinician_Name'].isin(Clinician_list), 'Manager'] = manager\n",
    "\n",
    "    \n",
    "    # Drop rows with NaN values in the 'region' column\n",
    "    Margin_df.dropna(subset=['region'], inplace=True)\n",
    "    \n",
    "    grouped_CC_ByRegion = Margin_df.groupby(['region', 'Manager'])\n",
    "    return Regional_Dashboard, unmatched_providers, matched_providers, grouped_CC_ByRegion,Margin_df\n",
    "\n",
    "\n",
    "PR_df = pd.read_excel(r\"D:\\payroll\\PR_PayrollRegister_KYB_31 (2).xls\",sheet_name=\"Payroll Register\",header=None)\n",
    "FDR_df = pd.read_excel(r\"D:\\payroll\\Financial Dashboard Report by Provider as of August 11 2025, 7-39 AM.xlsx\")\n",
    "PR_df, FDR_df = preprocess_PR_FDR_xls(PR_df, FDR_df)\n",
    "charge_capture_df = pd.read_excel(r\"D:\\payroll\\Charge_Capture_-_Jul_20th_-_Aug_2nd.xlsx\")\n",
    "company_roaster = pd.read_excel(r\"D:\\payroll\\Company_Roster_1754578475.xlsx\", skiprows=2)\n",
    "Regional_Dashboard, unmatched_providers, matched_providers, grouped_CC_ByRegion,Margin_df = build_metadata(charge_capture_df, company_roaster,PR_df, FDR_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Margin_df to Excel with autosized columns and wrapped text\n",
    "output_path = r\"D:\\payroll\\Margin_df.xlsx\"\n",
    "\n",
    "# Make a safe copy and stringify any list-like cells\n",
    "_to_str = lambda v: \", \".join(map(str, v)) if isinstance(v, (list, tuple)) else v\n",
    "_margin_out = Margin_df.copy()\n",
    "for col in _margin_out.select_dtypes(include=[\"object\"]).columns:\n",
    "    _margin_out[col] = _margin_out[col].map(_to_str)\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "    _margin_out.to_excel(writer, sheet_name=\"Margin\", index=False)\n",
    "    wb  = writer.book\n",
    "    ws  = writer.sheets[\"Margin\"]\n",
    "\n",
    "    header_fmt = wb.add_format({\"bold\": True, \"text_wrap\": True, \"valign\": \"top\", \"fg_color\": \"#DDEBF7\", \"border\": 1})\n",
    "    wrap_fmt   = wb.add_format({\"text_wrap\": True, \"valign\": \"top\"})\n",
    "    int_fmt    = wb.add_format({\"num_format\": \"#,##0\"})\n",
    "    float_fmt  = wb.add_format({\"num_format\": \"#,##0.00\"})\n",
    "\n",
    "    # Rewrite headers with format\n",
    "    for j, col in enumerate(_margin_out.columns):\n",
    "        ws.write(0, j, col, header_fmt)\n",
    "\n",
    "    # Autosize columns and set formats\n",
    "    for j, col in enumerate(_margin_out.columns):\n",
    "        ser = _margin_out[col]\n",
    "        max_len = max(len(str(col)), int(ser.astype(str).str.len().quantile(0.95) if len(ser) else 0)) + 2\n",
    "        width = min(max_len, 60)\n",
    "\n",
    "        fmt = None\n",
    "        if pd.api.types.is_integer_dtype(ser):\n",
    "            fmt = int_fmt\n",
    "        elif pd.api.types.is_float_dtype(ser):\n",
    "            fmt = float_fmt\n",
    "        else:\n",
    "            fmt = wrap_fmt\n",
    "\n",
    "        ws.set_column(j, j, width, fmt)\n",
    "\n",
    "    # Freeze header and add autofilter\n",
    "    ws.freeze_panes(1, 1)\n",
    "    ws.autofilter(0, 0, len(_margin_out), len(_margin_out.columns) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6432468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regional_Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for manger, group in grouped_CC_ByRegion:\n",
    "    print(f\"Manager: {manger}\")\n",
    "    print(group[['Clinician_Name', 'Gross_Encounters', 'Gross_Consents', 'Drafted_Encounters', 'Collections', 'Anticipated_Collections', 'Regular_Pay', 'Regular_Margin', 'Gross_Pay', 'Net_Margin', 'Anticipated_Net_Margin']])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639fdbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "FDR_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13e2f792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_22012\\404690565.py:2: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  new_PR=pd.read_excel(\"D:\\payroll\\Payroll History (18) (1).xlsx\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "new_PR=pd.read_excel(\"D:\\payroll\\Payroll History (18) (1).xlsx\")\n",
    "FDR_df = pd.read_excel(r\"D:\\payroll\\Financial Dashboard Report by Provider as of August 11 2025, 7-39 AM.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cca1de0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPANY CODE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>FILE NUMBER</th>\n",
       "      <th>POSITION ID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>DIST #</th>\n",
       "      <th>PAY DATE</th>\n",
       "      <th>CHECK/VOUCHER NUMBER</th>\n",
       "      <th>GROSS PAY</th>\n",
       "      <th>TAKE HOME</th>\n",
       "      <th>...</th>\n",
       "      <th>VOLUNTARY DEDUCTION : LIS-LIFE INSURANCE</th>\n",
       "      <th>VOLUNTARY DEDUCTION : MA1-OTHER</th>\n",
       "      <th>VOLUNTARY DEDUCTION : MIS-MISCELLANEOUS</th>\n",
       "      <th>VOLUNTARY DEDUCTION : MP1-MEDICAL PRE-TA</th>\n",
       "      <th>VOLUNTARY DEDUCTION : VIS-VIS PRE TAX</th>\n",
       "      <th>TOTAL VOLUNTARY DEDUCTIONS</th>\n",
       "      <th>MEMO : B-HLTH PLAN V</th>\n",
       "      <th>MEMO : J</th>\n",
       "      <th>MEMO : X01-401K MAX EL</th>\n",
       "      <th>TOTAL MEMOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HJ7</td>\n",
       "      <td>Afzal, Ahzam</td>\n",
       "      <td>10005.0</td>\n",
       "      <td>HJ7010005</td>\n",
       "      <td>A-Active</td>\n",
       "      <td>1.0</td>\n",
       "      <td>06/20/2025</td>\n",
       "      <td>250001.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HJ7</td>\n",
       "      <td>Ahmad, Haaris</td>\n",
       "      <td>10025.0</td>\n",
       "      <td>HJ7010025</td>\n",
       "      <td>A-Active</td>\n",
       "      <td>1.0</td>\n",
       "      <td>06/20/2025</td>\n",
       "      <td>250002.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HJ7</td>\n",
       "      <td>Casey, Jennifer</td>\n",
       "      <td>52025.0</td>\n",
       "      <td>HJ7052025</td>\n",
       "      <td>A-Active</td>\n",
       "      <td>1.0</td>\n",
       "      <td>06/20/2025</td>\n",
       "      <td>250003.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HJ7</td>\n",
       "      <td>Kendall, Jamila</td>\n",
       "      <td>63.0</td>\n",
       "      <td>HJ7000063</td>\n",
       "      <td>A-Active</td>\n",
       "      <td>1.0</td>\n",
       "      <td>06/06/2025</td>\n",
       "      <td>230001.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HJ7</td>\n",
       "      <td>Lee, Patricia</td>\n",
       "      <td>68.0</td>\n",
       "      <td>HJ7000068</td>\n",
       "      <td>A-Active</td>\n",
       "      <td>1.0</td>\n",
       "      <td>06/06/2025</td>\n",
       "      <td>230002.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>2025.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HJ7</td>\n",
       "      <td>Lee, Patricia</td>\n",
       "      <td>68.0</td>\n",
       "      <td>HJ7000068</td>\n",
       "      <td>A-Active</td>\n",
       "      <td>1.0</td>\n",
       "      <td>06/20/2025</td>\n",
       "      <td>250004.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>3750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HJ7</td>\n",
       "      <td>Levernier, Jacqueline M</td>\n",
       "      <td>510341.0</td>\n",
       "      <td>HJ7510341</td>\n",
       "      <td>A-Active</td>\n",
       "      <td>1.0</td>\n",
       "      <td>06/20/2025</td>\n",
       "      <td>250005.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HJ7</td>\n",
       "      <td>Osto, Hamza</td>\n",
       "      <td>10004.0</td>\n",
       "      <td>HJ7010004</td>\n",
       "      <td>A-Active</td>\n",
       "      <td>1.0</td>\n",
       "      <td>06/20/2025</td>\n",
       "      <td>250006.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HJ7</td>\n",
       "      <td>Palao Suarez, Cynthia</td>\n",
       "      <td>510239.0</td>\n",
       "      <td>HJ7510239</td>\n",
       "      <td>A-Active</td>\n",
       "      <td>1.0</td>\n",
       "      <td>06/20/2025</td>\n",
       "      <td>250007.0</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>3500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HJ7</td>\n",
       "      <td>Pressburger, Korey</td>\n",
       "      <td>510139.0</td>\n",
       "      <td>HJ7510139</td>\n",
       "      <td>A-Active</td>\n",
       "      <td>1.0</td>\n",
       "      <td>06/20/2025</td>\n",
       "      <td>250008.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  COMPANY CODE                     NAME  FILE NUMBER POSITION ID    STATUS  \\\n",
       "0          HJ7             Afzal, Ahzam      10005.0   HJ7010005  A-Active   \n",
       "1          HJ7            Ahmad, Haaris      10025.0   HJ7010025  A-Active   \n",
       "2          HJ7          Casey, Jennifer      52025.0   HJ7052025  A-Active   \n",
       "3          HJ7          Kendall, Jamila         63.0   HJ7000063  A-Active   \n",
       "4          HJ7            Lee, Patricia         68.0   HJ7000068  A-Active   \n",
       "5          HJ7            Lee, Patricia         68.0   HJ7000068  A-Active   \n",
       "6          HJ7  Levernier, Jacqueline M     510341.0   HJ7510341  A-Active   \n",
       "7          HJ7              Osto, Hamza      10004.0   HJ7010004  A-Active   \n",
       "8          HJ7    Palao Suarez, Cynthia     510239.0   HJ7510239  A-Active   \n",
       "9          HJ7       Pressburger, Korey     510139.0   HJ7510139  A-Active   \n",
       "\n",
       "   DIST #    PAY DATE  CHECK/VOUCHER NUMBER  GROSS PAY  TAKE HOME  ...  \\\n",
       "0     1.0  06/20/2025              250001.0    25000.0    25000.0  ...   \n",
       "1     1.0  06/20/2025              250002.0     5000.0     5000.0  ...   \n",
       "2     1.0  06/20/2025              250003.0     2000.0     2000.0  ...   \n",
       "3     1.0  06/06/2025              230001.0     5000.0     5000.0  ...   \n",
       "4     1.0  06/06/2025              230002.0     2025.0     2025.0  ...   \n",
       "5     1.0  06/20/2025              250004.0     3750.0     3750.0  ...   \n",
       "6     1.0  06/20/2025              250005.0     2500.0     2500.0  ...   \n",
       "7     1.0  06/20/2025              250006.0    25000.0    25000.0  ...   \n",
       "8     1.0  06/20/2025              250007.0     3500.0     3500.0  ...   \n",
       "9     1.0  06/20/2025              250008.0     5000.0     5000.0  ...   \n",
       "\n",
       "   VOLUNTARY DEDUCTION : LIS-LIFE INSURANCE  VOLUNTARY DEDUCTION : MA1-OTHER  \\\n",
       "0                                       NaN                              NaN   \n",
       "1                                       NaN                              NaN   \n",
       "2                                       NaN                              NaN   \n",
       "3                                       NaN                              NaN   \n",
       "4                                       NaN                              NaN   \n",
       "5                                       NaN                              NaN   \n",
       "6                                       NaN                              NaN   \n",
       "7                                       NaN                              NaN   \n",
       "8                                       NaN                              NaN   \n",
       "9                                       NaN                              NaN   \n",
       "\n",
       "   VOLUNTARY DEDUCTION : MIS-MISCELLANEOUS  \\\n",
       "0                                      NaN   \n",
       "1                                      NaN   \n",
       "2                                      NaN   \n",
       "3                                      NaN   \n",
       "4                                      NaN   \n",
       "5                                      NaN   \n",
       "6                                      NaN   \n",
       "7                                      NaN   \n",
       "8                                      NaN   \n",
       "9                                      NaN   \n",
       "\n",
       "   VOLUNTARY DEDUCTION : MP1-MEDICAL PRE-TA  \\\n",
       "0                                       NaN   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "5                                       NaN   \n",
       "6                                       NaN   \n",
       "7                                       NaN   \n",
       "8                                       NaN   \n",
       "9                                       NaN   \n",
       "\n",
       "   VOLUNTARY DEDUCTION : VIS-VIS PRE TAX  TOTAL VOLUNTARY DEDUCTIONS  \\\n",
       "0                                    NaN                         NaN   \n",
       "1                                    NaN                         NaN   \n",
       "2                                    NaN                         NaN   \n",
       "3                                    NaN                         NaN   \n",
       "4                                    NaN                         NaN   \n",
       "5                                    NaN                         NaN   \n",
       "6                                    NaN                         NaN   \n",
       "7                                    NaN                         NaN   \n",
       "8                                    NaN                         NaN   \n",
       "9                                    NaN                         NaN   \n",
       "\n",
       "   MEMO : B-HLTH PLAN V  MEMO : J  MEMO : X01-401K MAX EL  TOTAL MEMOS  \n",
       "0                   NaN       NaN                 25000.0      25000.0  \n",
       "1                   NaN       NaN                  5000.0       5000.0  \n",
       "2                   NaN       NaN                  2000.0       2000.0  \n",
       "3                   NaN       NaN                  5000.0       5000.0  \n",
       "4                   NaN       NaN                  2025.0       2025.0  \n",
       "5                   NaN       NaN                  3750.0       3750.0  \n",
       "6                   NaN       NaN                  2500.0       2500.0  \n",
       "7                   NaN       NaN                 25000.0      25000.0  \n",
       "8                   NaN       NaN                  3500.0       3500.0  \n",
       "9                   NaN       NaN                  5000.0       5000.0  \n",
       "\n",
       "[10 rows x 51 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_PR.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c856cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_PR_FDR_xlsx(PR_df, FDR_df):\n",
    "\n",
    "    def _to_numeric_series(s: pd.Series) -> pd.Series:\n",
    "        cleaned = (\n",
    "            s.astype(str).str.strip()\n",
    "             .str.replace(r'[,\\s$]', '', regex=True)\n",
    "             .str.replace(r'^\\((.*)\\)$', r'-\\1', regex=True)\n",
    "        )\n",
    "        return pd.to_numeric(cleaned, errors='coerce')\n",
    "    PR_df = PR_df.loc[:, [\"NAME\",\"REGULAR EARNINGS\",\"GROSS PAY\"]].copy()\n",
    "    PR_df.dropna(subset=[\"NAME\"], inplace=True)\n",
    "    for row in PR_df.itertuples():\n",
    "        personnel = row.NAME\n",
    "        personnel = personnel.strip().split(\",\")\n",
    "        personnel = personnel[1].strip() + \" \" + personnel[0].strip()\n",
    "        PR_df.at[row.Index, 'NAME'] = personnel\n",
    "    new_PR_sum=PR_df.groupby(\"NAME\").agg({\"GROSS PAY\":\"sum\",\"REGULAR EARNINGS\":\"sum\"}).reset_index()\n",
    "    # rename columns\n",
    "    new_PR_sum = new_PR_sum.rename(columns={\"NAME\":\"Personnel\", \"REGULAR EARNINGS\":\"Earnings_Reg\", \"GROSS PAY\":\"Gross\"})\n",
    "    \n",
    "    # Ensure numeric types for payroll\n",
    "    new_PR_sum[\"Earnings_Reg\"] = _to_numeric_series(new_PR_sum[\"Earnings_Reg\"])\n",
    "    new_PR_sum[\"Gross\"] = _to_numeric_series(new_PR_sum[\"Gross\"])\n",
    "\n",
    "    # FDR cleanup and numeric (copy slice first)\n",
    "    FDR_df = FDR_df.loc[:, [\"PROVIDER\",\"NET CHARGES\",\"NET RECEIPTS\"]].copy()\n",
    "    FDR_df[\"NET CHARGES\"] = _to_numeric_series(FDR_df[\"NET CHARGES\"])\n",
    "    FDR_df[\"NET RECEIPTS\"] = _to_numeric_series(FDR_df[\"NET RECEIPTS\"])\n",
    "\n",
    "    FDR_df = FDR_df[~((FDR_df[\"NET CHARGES\"] == 0.0) & (FDR_df[\"NET RECEIPTS\"] == 0.0))]\n",
    "    FDR_df.drop_duplicates(subset=[\"PROVIDER\"], inplace=True)\n",
    "    new_PR_sum.drop_duplicates(subset=[\"Personnel\"], inplace=True)\n",
    "    return new_PR_sum, FDR_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7858ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_PR, FDR_df = preprocess_PR_FDR_xlsx(new_PR, FDR_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5177673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Personnel</th>\n",
       "      <th>Gross</th>\n",
       "      <th>Earnings_Reg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abdulilah M Obeid</td>\n",
       "      <td>6480.00</td>\n",
       "      <td>6480.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adam James Geller</td>\n",
       "      <td>10952.07</td>\n",
       "      <td>10000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adejimi O Carrington</td>\n",
       "      <td>13749.00</td>\n",
       "      <td>13749.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ahmad Almawaldi</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>10000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ahmed Elgamal</td>\n",
       "      <td>9230.78</td>\n",
       "      <td>9230.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Vera Appiah-Dankwah</td>\n",
       "      <td>1350.00</td>\n",
       "      <td>1350.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Vibhuti Patel</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Victoria Francis</td>\n",
       "      <td>10384.62</td>\n",
       "      <td>10384.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Wanalee Chuasiriporn Betts</td>\n",
       "      <td>8636.43</td>\n",
       "      <td>5076.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Waris Hussain</td>\n",
       "      <td>6923.08</td>\n",
       "      <td>6923.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Personnel     Gross  Earnings_Reg\n",
       "0             Abdulilah M Obeid   6480.00       6480.00\n",
       "1             Adam James Geller  10952.07      10000.00\n",
       "2          Adejimi O Carrington  13749.00      13749.00\n",
       "3               Ahmad Almawaldi  10000.00      10000.00\n",
       "4                 Ahmed Elgamal   9230.78       9230.78\n",
       "..                          ...       ...           ...\n",
       "114         Vera Appiah-Dankwah   1350.00       1350.00\n",
       "115               Vibhuti Patel      0.00          0.00\n",
       "116            Victoria Francis  10384.62      10384.62\n",
       "117  Wanalee Chuasiriporn Betts   8636.43       5076.93\n",
       "118               Waris Hussain   6923.08       6923.08\n",
       "\n",
       "[119 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97f9f774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Personnel</th>\n",
       "      <th>Earnings_Reg</th>\n",
       "      <th>Gross</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olumide Adeyemo</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>5000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ahmad Almawaldi</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>5000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vera Appiah-Dankwah</td>\n",
       "      <td>720.00</td>\n",
       "      <td>720.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tamara Baird</td>\n",
       "      <td>2792.70</td>\n",
       "      <td>2792.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hana Bazzi</td>\n",
       "      <td>5038.47</td>\n",
       "      <td>5038.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Jeanne Wilson</td>\n",
       "      <td>4500.00</td>\n",
       "      <td>4500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Charles Joseph Wolf</td>\n",
       "      <td>4500.00</td>\n",
       "      <td>4500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Catherine Yergenson</td>\n",
       "      <td>4615.38</td>\n",
       "      <td>4615.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Revana Yousif</td>\n",
       "      <td>3461.54</td>\n",
       "      <td>3461.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Connie E Zurski</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>5544.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Personnel  Earnings_Reg    Gross\n",
       "0        Olumide Adeyemo       5000.00  5000.00\n",
       "1        Ahmad Almawaldi       5000.00  5000.00\n",
       "2    Vera Appiah-Dankwah        720.00   720.00\n",
       "4           Tamara Baird       2792.70  2792.70\n",
       "6             Hana Bazzi       5038.47  5038.47\n",
       "..                   ...           ...      ...\n",
       "124        Jeanne Wilson       4500.00  4500.00\n",
       "125  Charles Joseph Wolf       4500.00  4500.00\n",
       "126  Catherine Yergenson       4615.38  4615.38\n",
       "127        Revana Yousif       3461.54  3461.54\n",
       "128      Connie E Zurski       5000.00  5544.04\n",
       "\n",
       "[102 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PR_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741f50e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
